\documentclass{article}
\usepackage{amsmath,amssymb,amsthm} % AMS styles for extra equation formatting
\usepackage{graphicx} % for including graphics files
\usepackage{subfig} % for subfigures
\usepackage[numbers,sort]{natbib} % for better references control
\usepackage{hyperref} % for hyperlinks within the paper and references
\usepackage{fontspec}  % Allows for system fonts
\usepackage[top=2cm, bottom=2cm, left=2cm, right=2cm]{geometry}  % Set margins on all sides
\usepackage{setspace} % for line spacing
\usepackage{appendix} % for the appendices
\usepackage{listings} % for code
\usepackage{xcolor} % for color
\usepackage{url,textcomp}
\usepackage{matlab-prettifier}
\usepackage{tabularx}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\hypersetup{colorlinks=true, linkcolor=blue,  anchorcolor=blue,
citecolor=blue, filecolor=blue, menucolor=blue, pagecolor=blue,
urlcolor=blue}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\todo}[1]{\vspace{5 mm}\par \noindent
\marginpar{\textsc{Todo}}
\framebox{\begin{minipage}[c]{0.90 \textwidth}
\tt \flushleft #1 \end{minipage}}\vspace{5 mm}\par}
\newcommand{\setParDis}{\setlength {\parskip} {0.2cm} } % for 0.3cm spacing
\newcommand{\setParDef}{\setlength {\parskip} {0pt} } % for 0 spacing

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\graphicspath{{graphics/}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}

%\renewcommand{\qedsymbol}{$\blacksquare$} % for filled square at end of proof
%\numberwithin{equation}{section} % for the 1.1, 1.2 equation number style
%\setlength{\parindent}{0em} % don't indent paragraphs
%\setlength{\parskip}{1em} % add spacing between paragraphs
%\linespread{1.6} % double-spacing

\setmainfont{Arial}
% \doublespacing
\onehalfspacing
\setcounter{secnumdepth}{3}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\title{%
Report for Speech Recognition \\
\large\itshape{EEEM030 - Speech Recognition - Group Assignment 2}}
\author{\normalsize\slshape{Xiaoguang Liang}}
\date{\normalsize\slshape\today}
\maketitle


% Suppress any floats (figures, tables) from appearing on the next page
\suppressfloats

\tableofcontents

\begin{abstract}


\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\setParDis

This report is structured as follows: \textit{Section 2} analyses the kinematics of the Trossen PincherX-100 Robot. \textit{Section 3} compares the differences between Trossen PincherX-100 Robot and TiRobot II. \textit{Section 4} explores the applications of TiRobot II in detail. Finally, conclusions are drawn in \textit{Section 5}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{MFCC acoustic features}


\section{Model initialization}


\subsection{Paremeters of HMM initialization}
Model initialization is a foundational step in training Hidden Markov Models (HMMs). The success of training algorithms like the Baum-Welch algorithm relies heavily on well-initialized model parameters. These parameters include the initial state probabilities $\pi$, the state transition probabilities $A$, and the observation probabilities $B$. Proper initialization provides a good starting point for optimization and helps avoid convergence to suboptimal solutions. 


\paragraph{Initial State Probabilities $\Pi$:}
	 $\Pi = [\pi_1, \pi_2, \dots, \pi_N]$ represents the probability of starting in each state  i , where  N  is the number of states.
Here, initial $\Pi$ is provided in the assigment document:
\begin{equation}
\label{eqn:pi}
\pi_i = \begin{bmatrix}
 	1 & 0 & 0 & 0 & 0 & 0 & 0 & 0
 \end{bmatrix}
\end{equation}

\paragraph{State Transition Probabilities $A$:}
 $A = [a_{ij}]$  represents the probability of transitioning from state $i$ to state $j$, satisfying $\sum_{j=1}^N a_{ij} = 1$  for all $i$.
Here, initial $A$ is provided in the assigment document:
\begin{equation}
\label{eqn:A}
a_{ij} = \begin{bmatrix}
 	0.8 & 0.2 & 0 & 0 & 0 & 0 & 0 & 0 \\
 	0 & 0.8 & 0.2 & 0 & 0 & 0 & 0 & 0 \\
 	0 & 0 & 0.8 & 0.2 & 0 & 0 & 0 & 0 \\
 	0 & 0 & 0 & 0.8 & 0.2 & 0 & 0 & 0 \\
 	0 & 0 & 0 & 0 & 0.8 & 0.2 & 0 & 0 \\
 	0 & 0 & 0 & 0 & 0 & 0.8 & 0.2 & 0 \\
 	0 & 0 & 0 & 0 & 0 & 0 & 0.8 & 0.2 \\
 	0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.8
 \end{bmatrix}
\end{equation}

\paragraph{Observation Probabilities $B$:}
 $B$ represents the probability of observing data given a state. For continuous observations, this is computed using a multivariate Gaussian probability density function $PDF$.
 
 The observation probability for a given state $i$ and observation $x$ is defined as:
\begin{equation}
B_i(x) = \frac{1}{(2\pi)^{d/2} |\Sigma_i|^{1/2}} e^{-\frac{1}{2}(x - \mu_i)^T \Sigma_i^{-1} (x - \mu_i)}
\end{equation}

where:
\begin{itemize}
\item $\mu_i$ is the mean vector for state  i ,
\item $\Sigma_i$ is the covariance matrix for state  i ,
\item $d$ is the dimensionality of the observation vector.
\end{itemize}

In practice:
\begin{itemize}
\item $\mu_i$ can be initialized as the global mean of the observations.
\item $\Sigma_i$ can be initialized as a diagonal matrix where diagonal entries are the variances of the clustered observations.
\end{itemize}

\subsection{Initializaton for Multivariate Gaussian pdf}
In MMs with multivariate Gaussian probability density functions (PDFs), proper initialization of model parameters is critical for the success of the training process, particularly when using iterative algorithms like the Baum-Welch algorithm. Initialization provides the model with starting estimates for the emission probabilities, which are represented as Gaussian distributions, as well as the transition probabilities and initial state probabilities.

The initialization of the multivariate Gaussian PDF involves estimating the mean vector $\mu$ and the covariance matrix $\Sigma$ for each state in the HMM. These parameters characterize the probability distribution of the observed data within each state.

\subsubsection{Segments splitting for data}
Divide the length of the trainning samples evenly into $N$ parts, where $N = 8$, representing the number of hidden states. This converts the training samples into an $8N\times13$matrix, facilitating the computation of the Multivariate Gaussian PDF.

\subsubsection{Calculation for mean}


\subsubsection{Calculation for variance}


\section{Model training with HMMs}


\section{Evaluation}


\section{Conclusion}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\doi}[1]{DOI: \href{http://dx.doi.org/#1}{\nolinkurl{#1}}}
\bibliographystyle{ieeetr}
\bibliography{refs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
